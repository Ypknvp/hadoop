Here are the complete steps to install **Hadoop 3.2.1** on a Windows 10 system with a **Linux-like setup** (using tools like WSL or virtual machine):

---

### **1. Install Prerequisites**
#### **Install Java**
Hadoop requires Java to run.

1. Open your Linux terminal.
2. Update the package list:
   ```bash
   sudo apt update
   ```
3. Install Java Development Kit (JDK):
   ```bash
   sudo apt install openjdk-8-jdk -y
   ```
4. Verify Java installation:
   ```bash
   java -version
   ```
   Ensure it outputs Java 1.8.x.

5. Set the `JAVA_HOME` environment variable:
   ```bash
   echo "export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64" >> ~/.bashrc
   echo "export PATH=\$JAVA_HOME/bin:\$PATH" >> ~/.bashrc
   source ~/.bashrc
   ```

---

#### **Install SSH**
Hadoop requires SSH for communication.

1. Install SSH:
   ```bash
   sudo apt install openssh-server openssh-client -y
   ```
2. Start the SSH service:
   ```bash
   sudo service ssh start
   ```

---

### **2. Download Hadoop**
1. Download Hadoop 3.2.1:
   ```bash
   wget https://downloads.apache.org/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz
   ```
2. Extract the downloaded archive:
   ```bash
   tar -xvzf hadoop-3.2.1.tar.gz
   ```
3. Move Hadoop to a permanent directory:
   ```bash
   sudo mv hadoop-3.2.1 /usr/local/hadoop
   ```

---

### **3. Configure Hadoop**
#### **Update Environment Variables**
1. Open the `~/.bashrc` file:
   ```bash
   nano ~/.bashrc
   ```
2. Add the following lines:
   ```bash
   export HADOOP_HOME=/usr/local/hadoop
   export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
   export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
   export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
   ```
3. Apply changes:
   ```bash
   source ~/.bashrc
   ```

---

#### **Edit Configuration Files**
1. Navigate to the Hadoop configuration directory:
   ```bash
   cd $HADOOP_HOME/etc/hadoop
   ```
2. Update the following files:

**`core-site.xml`:**
```xml
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://localhost:9000</value>
  </property>
</configuration>
```

**`hdfs-site.xml`:**
```xml
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>1</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:///usr/local/hadoop/hdfs/namenode</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>file:///usr/local/hadoop/hdfs/datanode</value>
  </property>
</configuration>
```

**`mapred-site.xml`:**
- Copy the template:
  ```bash
  cp mapred-site.xml.template mapred-site.xml
  ```
- Update it:
  ```xml
  <configuration>
    <property>
      <name>mapreduce.framework.name</name>
      <value>yarn</value>
    </property>
  </configuration>
  ```

**`yarn-site.xml`:**
```xml
<configuration>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
</configuration>
```

---

### **4. Format HDFS**
1. Create necessary directories:
   ```bash
   sudo mkdir -p /usr/local/hadoop/hdfs/namenode
   sudo mkdir -p /usr/local/hadoop/hdfs/datanode
   sudo chown -R $USER:$USER /usr/local/hadoop/hdfs
   ```
2. Format the namenode:
   ```bash
   hdfs namenode -format
   ```

---

### **5. Start Hadoop Services**
1. Start HDFS:
   ```bash
   start-dfs.sh
   ```
2. Start YARN:
   ```bash
   start-yarn.sh
   ```

---

### **6. Verify Installation**
1. Open the following in your browser:
   - NameNode: [http://localhost:9870](http://localhost:9870)
   - ResourceManager: [http://localhost:8088](http://localhost:8088)

2. Check running services:
   ```bash
   jps
   ```
   You should see services like `NameNode`, `DataNode`, `ResourceManager`, and `NodeManager`.

---

### **7. Run a Word Count Example**
1. Create an input directory in HDFS:
   ```bash
   hdfs dfs -mkdir /input
   ```
2. Add a sample text file:
   ```bash
   echo "Hadoop is a distributed computing platform" > sample.txt
   hdfs dfs -put sample.txt /input/
   ```
3. Run the WordCount example:
   ```bash
   hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar wordcount /input /output
   ```
4. View the result:
   ```bash
   hdfs dfs -cat /output/part-r-00000
   ```

---

Now you have Hadoop 3.2.1 running on your Linux setup in Windows 10. ðŸŽ‰
